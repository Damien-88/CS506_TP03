{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHLcriKWLRe4"
      },
      "source": [
        "## Intro to PyTorch Tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Learning Objectives:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvJBqX8_Bctk"
      },
      "source": [
        "Understand the Core Concepts of PyTorch\n",
        "- Describe the purpose and architecture of PyTorch.\n",
        "- Identify key modules and their roles in the deep learning workflow.\n",
        "\n",
        "Work with Tensors\n",
        "- Create and manipulate tensors for data representation.\n",
        "- Understand tensor operations and GPU acceleration.\n",
        "\n",
        "Load and Preprocess Datasets\n",
        "- Use torchvision.datasets to load standard datasets such as MNIST.\n",
        "- Apply data transforms and preprocessing pipelines effectively.\n",
        "\n",
        "Build Neural Networks\n",
        "- Understand the basics of neural network architecture.\n",
        "- Construct neural networks using torch.nn.Module.\n",
        "\n",
        "Train Neural Networks Using Autograd\n",
        "- Utilize torch.autograd for automatic differentiation.\n",
        "- Implement the training loop including forward and backward passes.\n",
        "\n",
        "Evaluate and Visualize Model Performance\n",
        "- Measure model accuracy on test datasets.\n",
        "- Visualize training progress and prediction results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### What is PyTorch?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIFJ83ZTBctl"
      },
      "source": [
        "PyTorch is an open-source deep learning framework developed by Facebook's AI Research lab. It provides a flexible and intuitive platform for researchers and practitioners to build, train, and deploy machine learning models. PyTorch is widely used for applications in computer vision, natural language processing, reinforcement learning, and more. It combines the computational power of TensorFlow with the intuitive interface of NumPy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this tutorial, we will explore PyTorch fundamentals by building a complete pipeline for image classification using the MNIST dataset. The MNIST dataset contains 70,000 grayscale images of handwritten digits (0 through 9). We will cover:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Tensor operations\n",
        "- Data loading and preprocessing\n",
        "- Neural network construction\n",
        "- Training using backpropagation (torch.autograd)\n",
        "- Evaluation and visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment Setup (Jupyter Notebook VS Extension)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Package Installation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# In the terminal type the following\n",
        "pip install torch           # Core PyTorch Library\n",
        "pip install torchvision     # Datasetes, Models, and Transforms\n",
        "pip install matplotlib      # Plotting and Visualization\n",
        "pip install numpy           # Numerical Operations and Interperability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All Library Imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_JOISVgmn9v"
      },
      "source": [
        "## Working with Tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In PyTorch, a tensor is the fundamental unit of data. It is a multi-dimensional array, conceptually similar to NumPy’s ndarray, but with additional capabilities including automatic differentiation and GPU acceleration. Tensors are used to represent input data, model parameters, and outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Just like scalars, vectors, and matrices, tensors can have varying dimensions:\n",
        "- 0D tensor: Scalar\n",
        "- 1D tensor: Vector\n",
        "- 2D tensor: Matrix\n",
        "- nD tensor: Generalized multi-dimensional array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Why Use Tensors?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tensors are the backbone of every PyTorch computation. They:\n",
        "- Support automatic gradient computation (autograd)\n",
        "- Run efficiently on both CPUs and GPUs\n",
        "- Interoperate with NumPy\n",
        "- Are flexible for mathematical operations used in neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Common Tensor Types in PyTorch:\n",
        "- FloatTensor: 32-bit float values (default)\n",
        "- LongTensor: 64-bit integer values\n",
        "- BoolTensor: Boolean values\n",
        "- Tensor: Alias for FloatTensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Basic Workflow with Tensors:\n",
        "- Creation: Define tensors from data or using utility functions\n",
        "- Operations: Perform arithmetic, indexing, reshaping, and more\n",
        "- Device Transfer: Move tensors to GPU or back to CPU as needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Use Case Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Create a 1D tensor\n",
        "x = torch.tensor([1.0, 2.0, 3.0])\n",
        "print(\"1D Tensor:\", x)\n",
        "\n",
        "# Create tensors of different types and shapes\n",
        "x_zeros = torch.zeros((2, 3))         # 2x3 tensor filled with 0s\n",
        "x_ones = torch.ones((2, 3))           # 2x3 tensor filled with 1s\n",
        "x_rand = torch.rand((2, 3))           # Random values between 0 and 1\n",
        "\n",
        "# Tensor operations\n",
        "x_sum = x + 2                         # Add scalar to tensor\n",
        "x_mul = x * 3                         # Multiply each element\n",
        "\n",
        "print(\"Added tensor:\", x_sum)\n",
        "print(\"Multiplied tensor:\", x_mul)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Working with GPUs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    x_gpu = x.to('cuda')\n",
        "    print(\"Tensor on GPU:\", x_gpu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If CUDA is available, PyTorch allows you to transfer tensors to the GPU for faster computation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daQreKXIUslr"
      },
      "source": [
        "## Loading Datasets Using torchvision.datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjnAk1xcU0yc"
      },
      "source": [
        "#### What Are Datasets?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In machine learning, a dataset is a collection of labeled data used for training, validating, and testing models. PyTorch provides an interface for standard datasets through the torchvision.datasets module.\n",
        "The MNIST dataset, used here, is a popular benchmark consisting of 70,000 grayscale images of handwritten digits (0–9). Each image is 28x28 pixels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### What Are Datasets Used For?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Serve as the input to neural networks\n",
        "- Provide structured access to features and labels\n",
        "- Allow easy integration with data loaders for batching\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Types of Datasets:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Built-in Datasets: MNIST, CIFAR-10, ImageNet, etc.\n",
        "- Custom Datasets: Defined by subclassing torch.utils.data.Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Typical Workflow:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Define any transforms (preprocessing steps)\n",
        "- Instantiate the dataset (download if necessary)\n",
        "- Wrap with a DataLoader for efficient iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Use Case Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Step 1: Define preprocessing transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),               # Convert PIL image to tensor\n",
        "    transforms.Normalize((0.5,), (0.5,)) # Normalize pixel values\n",
        "])\n",
        "\n",
        "# Step 2: Download and load the datasets\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "# Step 3: Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each element returned by the dataset is a tuple: (image_tensor, label)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transforms and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### What are Transfomers?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Transforms are preprocessing functions that operate on images before they are used to train a neural network. They ensure that the input data is in the correct format, scaled properly, and optionally augmented to improve model generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Why Use Transforms?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Convert image formats (e.g., PIL to tensor)\n",
        "- Normalize values for stability during training\n",
        "- Augment data (e.g., flipping, rotation) to reduce overfitting\n",
        "- Resize images to a consistent input shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Types of Transforms:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Conversion: transforms.ToTensor()\n",
        "- Normalization: transforms.Normalize(mean, std)\n",
        "- Resizing: transforms.Resize(), transforms.CenterCrop()\n",
        "- Augmentation: transforms.RandomHorizontalFlip(), transforms.RandomRotation()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Typical Transform Pipeline:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Resize: Ensures all images are 28x28\n",
        "- ToTensor: Converts image to a tensor with shape (1, 28, 28) and values between 0 and 1\n",
        "- Normalize: Standardizes pixel values to be in the range [-1, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((28, 28)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Use Case Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFZ42Uq7UFDj"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Define a preprocessing pipeline\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((28, 28)),            # Resize images to 28x28\n",
        "    transforms.ToTensor(),                  # Convert to tensor\n",
        "    transforms.Normalize((0.5,), (0.5,))    # Normalize\n",
        "])\n",
        "\n",
        "# Apply during dataset loading\n",
        "mnist_train = datasets.MNIST(root='./data', train=True, transform=transform, download=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5ouUp1cU6pC"
      },
      "source": [
        "## Introduction to Neural Networks in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### What is a Neural Network?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A neural network is a computational model inspired by the structure and function of the human brain. It consists of layers of interconnected nodes (neurons), where each connection has an associated weight. Neural networks are capable of learning complex patterns from data through a process of optimization, allowing them to approximate nonlinear functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Why Use Neural Networks?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Neural networks are highly versatile and can model nonlinear relationships, making them effective for tasks like:\n",
        "- Image classification\n",
        "- Natural language processing\n",
        "- Speech recognition\n",
        "- Time series prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Components of a Neural Network:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Input Layer: Takes in raw features (e.g., image pixels).\n",
        "- Hidden Layers: Intermediate layers that apply transformations via activation functions.\n",
        "- Output Layer: Produces the final prediction (e.g., class probabilities)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each neuron performs a weighted sum of its inputs, applies an activation function, and passes the output forward to the next layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Activation Functions:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- ReLU (Rectified Linear Unit): Outputs zero if input is negative, else outputs input itself. It is efficient and widely used in hidden layers.\n",
        "- Sigmoid: Maps input values between 0 and 1, useful for binary classification.\n",
        "- Softmax: Converts a vector of raw scores into probabilities that sum to 1, typically used in the output layer for multi-class classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Types of Neural Networks:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Fully Connected (Dense) Networks: Every neuron in one layer is connected to every neuron in the next layer. Suitable for tabular data and simple image tasks.\n",
        "- Convolutional Neural Networks (CNNs): Use convolutional layers to capture spatial hierarchies in image data, widely used in computer vision.\n",
        "- Recurrent Neural Networks (RNNs): Designed for sequential data like time series or text by maintaining memory of previous inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Typical Workflow:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Define model architecture (layers, activation functions)\n",
        "- Initialize weights and biases\n",
        "- Forward propagate input through layers\n",
        "- Compute loss comparing predictions with true labels\n",
        "- Backpropagate gradients through the network (torch.autograd)\n",
        "- Update parameters using an optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building a Neural Network with torch.nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "torch.nn.Module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PyTorch provides a module called torch.nn for building neural networks. The nn.Module class is the base class for all neural network models. You define your architecture by subclassing nn.Module and implementing:\n",
        "- init: Define layers\n",
        "- forward: Define forward pass logic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Types of Layers:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Linear: nn.Linear(in_features, out_features)\n",
        "- Activation Functions: nn.ReLU, nn.Sigmoid, nn.Softmax\n",
        "- Loss Functions: nn.CrossEntropyLoss, nn.MSELoss\n",
        "- Sequential Containers: nn.Sequential(...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Use Case Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define a simple 2-layer neural network for MNIST\n",
        "class MNISTModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNISTModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 128)  # Input layer (flattened 28x28 image)\n",
        "        self.fc2 = nn.Linear(128, 64)     # Hidden layer\n",
        "        self.fc3 = nn.Linear(64, 10)      # Output layer (10 classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)             # Flatten image to vector\n",
        "        x = F.relu(self.fc1(x))           # Activation 1\n",
        "        x = F.relu(self.fc2(x))           # Activation 2\n",
        "        x = self.fc3(x)                   # Output (raw scores/logits)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Instantiating the Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = MNISTModel()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model summary shows the layers and their dimensions, confirming that the network is ready for training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training the Neural Network with torch.autograd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### What is torch.autograd?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "torch.autograd is PyTorch’s automatic differentiation engine. It tracks all tensor operations and builds a computation graph on-the-fly, which is then used to compute gradients during backpropagation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Why Use autograd?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Automatically computes gradients\n",
        "- Enables efficient backpropagation\n",
        "- Essential for training neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Typical Training Workflow:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Forward Pass: Compute predictions\n",
        "- Loss Calculation: Measure error\n",
        "- Zero Gradients: Reset gradients from previous step\n",
        "- Backward Pass: Use loss.backward() to compute gradients\n",
        "- Update Weights: Apply optimizer step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Use Case Example (Full Training Loop):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Step 1: Initialize model, loss function, and optimizer\n",
        "model = MNISTModel()\n",
        "criterion = nn.CrossEntropyLoss()                 # Suitable for classification\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Step 2: Training loop\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()     # Zero previous gradients\n",
        "        loss.backward()           # Compute new gradients\n",
        "        optimizer.step()          # Update weights\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notes:\n",
        "- loss.backward() computes the gradients of the loss w.r.t. each model parameter.\n",
        "- optimizer.step() updates the parameters using those gradients.\n",
        "- Always call optimizer.zero_grad() before the backward pass to avoid gradient accumulation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Evaluation and Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Why Evaluate?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After training, we evaluate the model on the test set to assess its generalization. This involves:\n",
        "- Running a forward pass without gradient computation\n",
        "- Comparing predicted classes to actual labels\n",
        "- Calculating accuracy or other metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Use Case Example (Model Evaluation):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "model.eval()  # Set model to evaluation mode\n",
        "\n",
        "with torch.no_grad():  # Disable gradient calculation\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Why Visualize?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualizing predictions helps us:\n",
        "- Interpret model behavior\n",
        "- Identify misclassified examples\n",
        "- Debug data or model issues"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.\tConfusion Matrix\n",
        "Shows the performance of classification models by summarizing true vs. predicted labels. Helps diagnose types of errors such as false positives and false negatives.\n",
        "\n",
        "2.\tDecision Boundary Plots\n",
        "Visualize the regions in feature space where a classifier assigns different classes. Useful for understanding how a model separates different classes.\n",
        "\n",
        "3.\tFeature Importance / Coefficients Plots\n",
        "Show which features contribute most to the model's decisions (common in tree-based or linear models).\n",
        "\n",
        "4.\tLearning Curves\n",
        "Show model performance as a function of training set size, helping diagnose underfitting or overfitting.\n",
        "\n",
        "5.\tResidual Plots\n",
        "Common in regression, they visualize errors to check model assumptions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Use Case Example (Display Predictions):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Display 6 predictions\n",
        "examples = enumerate(test_loader)\n",
        "batch_idx, (images, labels) = next(examples)\n",
        "outputs = model(images)\n",
        "_, preds = torch.max(outputs, 1)\n",
        "\n",
        "for i in range(6):\n",
        "    plt.subplot(2, 3, i+1)\n",
        "    plt.imshow(images[i][0], cmap='gray')\n",
        "    plt.title(f\"Label: {labels[i]}, Pred: {preds[i]}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion and Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this tutorial, you’ve learned how to use PyTorch to build, train, and evaluate a neural network for image classification using the MNIST dataset. Here's what we covered:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Next Steps:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Understanding and manipulating tensors\n",
        "- Loading and transforming datasets\n",
        "- Building neural networks using torch.nn\n",
        "- Training using torxh.autograd and backpropagation\n",
        "- Evaluating performance and visualizing results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., ... & Chintala, S. (2019). PyTorch: An imperative style, high-performance deep learning library. Advances in Neural Information Processing Systems, 32, 8026-8037. https://arxiv.org/abs/1912.01703\n",
        "\n",
        "Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT Press. https://www.deeplearningbook.org/\n",
        "\n",
        "LeCun, Y., Cortes, C., & Burges, C. J. C. (1998). The MNIST database of handwritten digits. http://yann.lecun.com/exdb/mnist/\n",
        "\n",
        "Torchvision contributors. (2023). torchvision.datasets — PyTorch 2.0.1 documentation. https://pytorch.org/vision/stable/datasets.html\n",
        "\n",
        "Torchvision contributors. (2023). torchvision.transforms — PyTorch 2.0.1 documentation. https://pytorch.org/vision/stable/transforms.html\n",
        "\n",
        "Paszke, A., et al. (2017). Automatic differentiation in PyTorch. NIPS Autodiff Workshop. https://openreview.net/pdf?id=BJJsrmfCZ\n",
        "\n",
        "PyTorch. (2023). PyTorch documentation. https://pytorch.org/docs/stable/index.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Code Source References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PyTorch:\n",
        "\n",
        "Paszke, A., et al. (2019). PyTorch: An imperative style, high-performance deep learning library. Advances in Neural Information Processing Systems, 32, 8026-8037. https://arxiv.org/abs/1912.01703\n",
        "\n",
        "Matplotlib:\n",
        "\n",
        "Hunter, J. D. (2007). Matplotlib: A 2D graphics environment. Computing in Science & Engineering, 9(3), 90–95. https://doi.org/10.1109/MCSE.2007.55\n",
        "\n",
        "NumPy:\n",
        "\n",
        "Harris, C. R., et al. (2020). Array programming with NumPy. Nature, 585(7825), 357-362. https://doi.org/10.1038/s41586-020-2649-2"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "JndnmDMp66FL",
        "YHIWvc9Ms-Ll",
        "TJffr5_Jwqvd"
      ],
      "name": "intro_to_pandas.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
